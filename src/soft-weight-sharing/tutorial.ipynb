{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tutorial on 'Soft weight-sharing for Neural Network compression' \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Recently, compression of neural networks has been a much-discussed issue.  One reason for this is the desire to run and store them on mobile devices such as smartphones, robots or [Rasberry Pis](https://github.com/samjabrahams/tensorflow-on-raspberry-pi). Another problem is the energy consumption of a multimillion parameter network. When inference is run at scale the costs can quickly add up.\n",
    "\n",
    "Compressing a given neural network is a challenging task. While the idea of storing weights with low precision has been around for some time, ideas to store the weight matrix in a different storage format are recent. \n",
    "The latter proposal has led to the most successful compression scheme so far. \n",
    "Under the assumption that there is great redundancy within the weights, one can prune most of them. Consequently, only non-zero weights are being stored. These weights can further be compressed by qunatizing them.\n",
    "However, it is not clear how to infer the redundant weights given a trained neural network or to quantize the remaining weights. In the following, we will identify the problem more clearly and review one recent attempt to tackle it. For the following illustrations, I work with MNIST and LeNet-300-100 a simple 2-fully connected neural network with 300 and 100 units.\n",
    "\n",
    "Usually, when training an unregularized neural network, the distribution of weights looks somewhat like a Normal distribution centered at zero. For the proposed storage format this is not ideal. We would like a distribution that is sharply (ideally delta) peaked around some values with significant mass in the zero peak.\n",
    "\n",
    "Weight distribution while common training  | |  Desired distribution    \n",
    ":-------------------------:|:------------:|:-------------------------:\n",
    "![](./figures/han_pretraining.gif \"title-1\")|<img src=\"./figures/arrow.jpg\" style=\"width: 200px;\"/>| ![](./figures/han_clustering.png \"title-1\")\n",
    "\n",
    "Following we will shortly review how [Han et. al. (2016)](https://arxiv.org/abs/1510.00149), the proposers of this compression format and current state-of-the-art in compression, tackle the problem. \n",
    "The authors use a multistage pipeline: (i) re-training a pre-trained network with Gaussian prior aka L2-norm on the weights (ii) repeatedly cutting off all weights around a threshold close to zero and after that continue training with L2-norm, and (iii) clustering all weights and retraining again the cluster means.\n",
    "\n",
    "(i)  Re-training with L2 regularization  |  (ii) Repetitiv Cutting and training                           \n",
    "         :-------------------------:|:-------------------------:\n",
    "![](./figures/han_retraining.gif \"title-1\")|![](./figures/han_cutandtrain.gif \"title-1\")\n",
    "**(ii) Final stage before clustering**  | **(iii)  Clustering**\n",
    "![](./figures/han_final_cut.png \"title-1\")|![](./figures/han_clustering.png \"title-1\")\n",
    "\n",
    "\n",
    "Note that this pipeline is not a differentiable function. Furthermore, pruning and quantization are distinct stages. \n",
    "\n",
    "In contrast, we propose to sparsify and cluster weights in one differentiable retraining procedure. More precisely, we train the network weights with a Gaussian mixture model prior. \n",
    "This is an instance of an empirical Bayesian prior because the parameters in the prior are being learned as well. \n",
    "With this prior present, weights will naturally cluster together since that will allow the gaussian mixture to lower the variance and thus achieve higher probability. \n",
    "It is important, to carefully initialize the learning procedure for those priors because one might end up in a situation where the weights \"chase\" the mixture and the mixture the weights. \n",
    "\n",
    "Note that, even though compression seems to be a core topic of information theory, so far there has been little attention on this angle on things. While in our paper the emphasis lays on this information theoretic view, here we will restrict ourselves to a somewhat practical one.\n",
    "\n",
    "Following, we give a tutorial that shall serve as a practical guide to implementing empirical priors and in particular a Gaussian Mixture with an Inverse-Gamma prior on the variances.  It is divided into 3 parts.\n",
    "\n",
    "\n",
    "* **PART 1:** Pretraining a Neural Network\n",
    "\n",
    "* **PART 2:** Re-train the network with an empirical Gaussian Mixture Prior with Inverse-Gamma hyper-prior on the variances. \n",
    "\n",
    "* **PART 3:** Post-process the re-trained network weights\n",
    "\n",
    "## PART 1: Pretraining a Neural Network \n",
    "\n",
    "\n",
    "First of all, we need a parameter heavy network to compress. In this first part of the tutorial, we train a simple -2 convolutional, 2 fully connected layer- neural network on MNIST with 642K paramters. \n",
    "___________________________\n",
    "\n",
    "We start by loading some essential libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "Following, we load the MNIST dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 60000 train samples and 10000 test samples.\n"
     ]
    }
   ],
   "source": [
    "from dataset import mnist\n",
    "\n",
    "[X_train, X_test], [Y_train, Y_test], [img_rows, img_cols], nb_classes = mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ___________________________________________________\n",
    "\n",
    "Next, we choose a model. We decide in favor of a  classical 2 convolutional, 2 fully connected layer network with ReLu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 25)        650       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 50)          11300     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               625500    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "_________________________________________________________________\n",
      "error_loss (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 642,460\n",
      "Trainable params: 642,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(25, (5, 5), strides=(2, 2), activation=\"relu\")`\n",
      "  \n",
      "/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(50, (3, 3), strides=(2, 2), activation=\"relu\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,  Activation, Flatten, Convolution2D\n",
    "\n",
    "# We configure the input here to match the backend. If properly done this is a lot faster. \n",
    "if K._BACKEND == \"theano\":\n",
    "    InputLayer = Input(shape=(1, img_rows, img_cols), name=\"input\")\n",
    "elif K._BACKEND == \"tensorflow\":\n",
    "    InputLayer = Input(shape=(img_rows, img_cols,1), name=\"input\")\n",
    "\n",
    "# A classical architecture ...\n",
    "#   ... with 3 convolutional layers,\n",
    "Layers = Convolution2D(25, 5, 5, subsample = (2,2), activation = \"relu\")(InputLayer)\n",
    "Layers = Convolution2D(50, 3, 3, subsample = (2,2), activation = \"relu\")(Layers)\n",
    "#   ... and 2 fully connected layers.\n",
    "Layers = Flatten()(Layers)\n",
    "Layers = Dense(500)(Layers)\n",
    "Layers = Activation(\"relu\")(Layers)\n",
    "Layers = Dense(nb_classes)(Layers)\n",
    "PredictionLayer = Activation(\"softmax\", name =\"error_loss\")(Layers)\n",
    "\n",
    "# Fianlly, we create a model object:\n",
    "model = Model(input=[InputLayer], output=[PredictionLayer])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------\n",
    "Next, we train the network for 100 epochs with the Adam optimizer. Let's see where our model gets us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer= opt,\n",
    "              loss = {\"error_loss\": \"categorical_crossentropy\",},\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit({\"input\": X_train, }, {\"error_loss\": Y_train},\n",
    "          nb_epoch = epochs, batch_size = batch_size,\n",
    "          verbose = 0, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: *** The model should end up with approx. 0.9% error rate.\n",
    "___________________________________\n",
    "Fianlly, we save the model in case we need to reload it later, e.g. if you want to play around with the code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model, \"./my_pretrained_net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________\n",
    "__________________________________________\n",
    "\n",
    "## PART 2: Re-training the network with an empirical prior\n",
    "\n",
    "_____________________________________________________\n",
    "\n",
    "First of all, we load our pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras.models.load_model(\"./my_pretrained_net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we will initialize a 16 component Gaussian mixture as our empirical prior. We will learn all parameters in the prior but the mean and the mixing proportion of the zero component, we set $\\mu_0=0$ and $\\pi_0=0.99$, respectively. Furthermore, we put a Gamma hyper-prior on the precisions of the Gaussian mixture. We set the mean such that the expected variance is $0.02$. The variance of the hyper-prior is an estimate of how strongly the variance is regularized. Note that, the variance of the zero component has much more data (i.e. weight) evidence than the other components thus we put a stronger prior on it. Somewhat counterintuitive we found it beneficial to have wider and thus noisier expected variances.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from empirical_priors import GaussianMixturePrior\n",
    "from extended_keras import extract_weights\n",
    "\n",
    "pi_zero = 0.99\n",
    "\n",
    "RegularizationLayer = GaussianMixturePrior(nb_components=16, \n",
    "                                           network_weights=extract_weights(model),\n",
    "                                           pretrained_weights=pretrained_model.get_weights(), \n",
    "                                           pi_zero=pi_zero,\n",
    "                                           name=\"complexity_loss\")(Layers)\n",
    "\n",
    "model = Model(input = [InputLayer], output = [PredictionLayer, RegularizationLayer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We optimize the network again with ADAM, the learning rates for the network parameters, means, variances and mixing proportions may differ though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimizers \n",
    "from extended_keras import identity_objective\n",
    "\n",
    "tau = 0.003\n",
    "N = X_train.shape[0] \n",
    "\n",
    "opt = optimizers.Adam(lr = [5e-4,1e-4,3e-3,3e-3],  #[unnamed, means, log(precition), log(mixing proportions)]\n",
    "                      param_types_dict = ['means','gammas','rhos'])\n",
    "\n",
    "model.compile(optimizer = opt,\n",
    "              loss = {\"error_loss\": \"categorical_crossentropy\", \"complexity_loss\": identity_objective},\n",
    "              loss_weights = {\"error_loss\": 1. , \"complexity_loss\": tau/N},\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our network for 30 epochs, each taking about 45s. You can watch the progress yourself. At each epoch, we compare the original weight distribution (histogram top) to the current distribution (log-scaled histogram right). The joint scatter plot in the middle shows how each weight changed.\n",
    "\n",
    "*Note* that we had to scale the histogram logarithmically otherwise it would be little informative due to the zero spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n",
      "/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected dimension in the range [0, 0), but got -1\n\t [[Node: metrics_2/acc_1/ArgMax_1 = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](complexity_loss/add_19, loss_2/error_loss_loss/Sum/reduction_indices)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-08cdde44a695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           verbose = 1., callbacks=[VisualisationCallback(model,X_test,Y_test, epochs)])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful_metric_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/Documents/github_projects/waffles-and-posteriors/src/soft-weight-sharing/extended_keras.pyc\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/Documents/github_projects/waffles-and-posteriors/src/soft-weight-sharing/extended_keras.pyc\u001b[0m in \u001b[0;36mplot_histogram\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             self.model.evaluate({'input': self.X_test, }, {\"error_loss\": self.Y_test, \"complexity_loss\": self.Y_test, },\n\u001b[0;32m--> 157\u001b[0;31m                                 verbose=0)[3]\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %d /%d\\nTest accuracy: %.4f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0max_marg_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1114\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akashrajkn/.conda/envs/ull/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected dimension in the range [0, 0), but got -1\n\t [[Node: metrics_2/acc_1/ArgMax_1 = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](complexity_loss/add_19, loss_2/error_loss_loss/Sum/reduction_indices)]]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from extended_keras import VisualisationCallback\n",
    "\n",
    "epochs = 30\n",
    "model.fit({\"input\": X_train,},\n",
    "          {\"error_loss\" : Y_train, \"complexity_loss\": np.zeros((N,1))},\n",
    "          nb_epoch = epochs,\n",
    "          batch_size = batch_size,\n",
    "          verbose = 1., callbacks=[VisualisationCallback(model,X_test,Y_test, epochs)])\n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(url='./figures/retraining.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Post-processing\n",
    "\n",
    "Now, the only thing that is left to do is setting each weight to the mean of the component that takes most responsibility for it i.e. quantising the weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import discretesize\n",
    "\n",
    "retrained_weights = np.copy(model.get_weights())\n",
    "compressed_weights = np.copy(model.get_weights())\n",
    "compressed_weights[:-3] = discretesize(compressed_weights, pi_zero = pi_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the accuracy of the reference, the retrained and the post-processed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODEL ACCURACY\")\n",
    "score = pretrained_model.evaluate({'input': X_test, },{\"error_loss\" : Y_test,}, verbose=0)[1]\n",
    "print(\"Reference Network: %0.4f\" %score)\n",
    "score = model.evaluate({'input': X_test, },{\"error_loss\" : Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Retrained Network: %0.4f\" %score)\n",
    "model.set_weights(compressed_weights)\n",
    "score = model.evaluate({'input': X_test, },{\"error_loss\" : Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Post-processed Network: %0.4f\" %score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let us see how many weights have been pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import special_flatten\n",
    "weights = special_flatten(compressed_weights[:-3]).flatten()\n",
    "print(\"Non-zero weights: %0.2f %%\" % (100.*np.count_nonzero(weights)/ weights.size) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this naive implementation we got rid of 19 out of 20 weights. Furthermore note that we quantize weights with only 16 cluster means (aka 4 bit indexes). \n",
    "\n",
    "For better results (up to 0.5%) one may anneal $\\tau$, learn the mixing proportion for the zero spike with a beta prior on it for example and ideally optimize with some hyperparamter optimization of choice such as spearmint (I also wrote some example code for deep learning and spearmint).\n",
    "\n",
    "We finish this tutorial with a series of histograms showing the results of our procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import save_histogram\n",
    "\n",
    "save_histogram(pretrained_model.get_weights(),save=\"figures/reference\")\n",
    "save_histogram(retrained_weights,save=\"figures/retrained\")\n",
    "save_histogram(compressed_weights,save=\"figures/post-processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Weight distribution before retraining | Weight distribution after retraining|  Weight distribution after post-processing  \n",
    ":-------------------------:|:-------------------------:|:------------:|:-------------------------:\n",
    "histogram|![](./figures/reference.png)|<img src=\"./figures/retrained.png\"/>| ![](./figures/post-processed.png)\n",
    "log-scaled histogram|![](./figures/reference_log.png)|<img src=\"./figures/retrained_log.png\"/>| ![](./figures/post-processed_log.png)\n",
    "_______________________________\n",
    "### *Reference*\n",
    "\n",
    "The paper \"Soft weight-sharing for Neural Network compression\" has been accepted to ICLR 2017.\n",
    "\n",
    "\n",
    "    @inproceedings{ullrich2017soft,\n",
    "    title={Soft Weight-Sharing for Neural Network Compression},\n",
    "    author={Ullrich, Karen and  Meeds, Edward and Welling, Max},\n",
    "    booktitle={ICLR 2017},\n",
    "    year={2017}\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
